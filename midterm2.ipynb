{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "midterm2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRfcjEF+pRs6H2li3U9tqD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tufts-mathmodeling/HW/blob/master/midterm2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAERFzRb-Dnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install BeautifulSoup4 lxml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kclpr4J7-GZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%config InlineBackend.figure_formats = ['retina']\n",
        "import json\n",
        "import datetime\n",
        "import requests\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from random import sample\n",
        "from urllib.parse import urlparse\n",
        "from collections import deque, defaultdict\n",
        "from bs4 import BeautifulSoup\n",
        "from numpy import linalg as LA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMlcxy6U-PJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USER_AGENT = ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) '\n",
        "              'Apple WebKit/537.36 (KHTML, like Gecko) '\n",
        "              'Chrome/80.0.3987.149 Safari/537.36')\n",
        "\n",
        "def unique_url(url):\n",
        "    \"\"\"Returns a canonical representation of a link.\"\"\"\n",
        "    stripped = url.replace('://www.', '://')\n",
        "    stripped = stripped.replace(f'{urlparse(url).scheme}://', '').rstrip('/')\n",
        "    if '#' in stripped:\n",
        "      # Remove trailing anchor\n",
        "      return '#'.join(stripped.split('#')[:-1])\n",
        "    return stripped\n",
        "\n",
        "\n",
        "def check_page(url):\n",
        "    \"\"\"Verifies that a URL corresponds to a reachable web page.\"\"\"\n",
        "    try:\n",
        "        head = requests.head(url,\n",
        "                             headers={'User-Agent': USER_AGENT},\n",
        "                             allow_redirects=True,\n",
        "                             timeout=1)\n",
        "        content_type = head.headers.get('Content-Type', '').lower()\n",
        "        return content_type.startswith('text/html')\n",
        "    except requests.exceptions.RequestException as ex:\n",
        "        # Couldn't load this particular page, for whatever reason (404, etc.)\n",
        "        return False\n",
        "\n",
        "\n",
        "def all_links(url):\n",
        "    \"\"\"Retrieves all links from a web page.\"\"\"\n",
        "    try:\n",
        "        req = requests.get(url,\n",
        "                           headers={'User-Agent': USER_AGENT},\n",
        "                           allow_redirects=True,\n",
        "                           timeout=5)\n",
        "    except requests.exceptions.RequestException as ex:\n",
        "        # Couldn't load this particular page, for whatever reason (404, etc.)\n",
        "        return None\n",
        "\n",
        "    # Get links from page.\n",
        "    soup = BeautifulSoup(req.text, 'lxml')\n",
        "    return [link.get('href', '') for link in soup.select('a')]\n",
        "\n",
        "\n",
        "def filter_links(source_url, urls, blacklist, whitelist,\n",
        "                 follow_relative_links):\n",
        "    \"\"\"Filters and cleans links.\"\"\"\n",
        "    filtered = []\n",
        "    o = urlparse(source_url)\n",
        "    source_scheme = o.scheme\n",
        "    source_domain = o.netloc\n",
        "\n",
        "    for href in urls:\n",
        "        if not href or href.startswith('#') or \\\n",
        "          (not href.startswith('http') and not follow_relative_links):\n",
        "            # Only accept absolute links if explicitly allowed;\n",
        "            # remove empty links and anchors\n",
        "            continue\n",
        "        else:\n",
        "            if href.startswith('//'):\n",
        "                href = 'https:' + href\n",
        "            elif href.startswith('/'):\n",
        "                # Internal link (w.r.t. website root)\n",
        "                href = f'{source_scheme}://{source_domain}{href}'\n",
        "            elif not href.startswith('http'):\n",
        "                # Internal link (w.r.t. current link)\n",
        "                href = source_url.rstrip('/') + '/' + href\n",
        "        if '#' in href:\n",
        "            href = '#'.join(href.split('#')[:-1]) # strip anchor\n",
        "\n",
        "        # Blacklist/whitelist filtering.\n",
        "        blacklisted = False\n",
        "        whitelisted = False\n",
        "        for keyword in blacklist:\n",
        "            if keyword.lower() in href.lower():\n",
        "                blacklisted = True\n",
        "                break\n",
        "        for keyword in whitelist:\n",
        "            if keyword.lower() in href.lower():\n",
        "                whitelisted = True\n",
        "                break\n",
        "        if ((whitelist and whitelisted) or not whitelist) and \\\n",
        "           ((blacklist and not blacklisted) or not blacklist) and \\\n",
        "           'mailto:' not in href and 'javascript:' not in href:\n",
        "            filtered.append(href)\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def crawl(start_url,\n",
        "          save_file,\n",
        "          n,\n",
        "          verbose=True,\n",
        "          follow_relative_links=True,\n",
        "          max_links_per_page=None,\n",
        "          blacklist=None,\n",
        "          whitelist=None):\n",
        "    \"\"\"Crawls pages starting from a given URL and saves the link graph.\n",
        "\n",
        "    :param start_url: The starting URL.\n",
        "    :param save_file: The JSON file to save crawl data to.\n",
        "    :param n: The number of pages to add to the graph. Pages in the graph have\n",
        "        not necessarily been visited (such pages never have outlinks).\n",
        "    :param verbose: Determines whether to print URLs as they are crawled.\n",
        "    :param follow_relative_links: Determines whether to follow relative links.\n",
        "        Following relative links tends to result in narrow, deep crawl--many\n",
        "        pages being crawled on a relatively small number of websites.\n",
        "    :param max_links_per_page: The number of links to add to the link queue\n",
        "        per page. (For each page, the order of links is always shuffled.)\n",
        "    :param blacklist: A blacklist of URL keywords. If any keyword on the\n",
        "        blacklist appears in a URL, it will be excluded from the graph and\n",
        "        not crawled.\n",
        "    :param whitelist: A whitelist of URL keywords. If the whitelist is nonempty,\n",
        "        a URL will only be included in the graph and crawled if at least one\n",
        "        keyword from the whitelist is included in it. The whitelist can be\n",
        "        combined with the blacklist for highly detailed URL filtering.\n",
        "    \"\"\"\n",
        "    enqueued = set([unique_url(start_url)])\n",
        "    dead = set([])\n",
        "    link_graph = []\n",
        "    link_queue = deque([start_url])\n",
        "    ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
        "    if max_links_per_page is None:\n",
        "        max_links_per_page = float('inf')\n",
        "    if blacklist is None:\n",
        "        blacklist = []\n",
        "    if whitelist is None:\n",
        "        whitelist = []\n",
        "\n",
        "    while link_queue and len(enqueued) < n:\n",
        "        # Breadth-first search: the traverse all the links on the first\n",
        "        # page crawled before moving on to links on the second page crawled,\n",
        "        # and so on.\n",
        "        curr_link = link_queue.popleft()\n",
        "        if verbose:\n",
        "            print(curr_link)\n",
        "\n",
        "        # Fetch page metadata and text.\n",
        "        if not check_page(curr_link):\n",
        "            # Page not reachable or content type incorrect\n",
        "            dead.add(curr_link)\n",
        "            continue\n",
        "        links = all_links(curr_link)\n",
        "        if links is None:\n",
        "            # GET request failed\n",
        "            dead.add(curr_link)\n",
        "            continue\n",
        "\n",
        "        # Filter and sample links from the page.\n",
        "        # Each filtered link has four possible states:\n",
        "        # (1) Visited. (The page was loaded and its links were processed.)\n",
        "        # (2) Dead. (A visit to the page was attempted but failed.)\n",
        "        # (3) Enqueued. (We intend to eventually visit the page, but we have\n",
        "        #                not loaded its contents yet.)\n",
        "        # (4) Unseen. (A page was linked to at some point, but we have not\n",
        "        #              loaded its contents and do not currently intend to.)\n",
        "        # Unseen pages are retained in the link graph because we might\n",
        "        # stumble across them again and decide to visit them after initially \n",
        "        # passing them over for visitation. Unseen pages that are not\n",
        "        # eventually visited are filtered out of the final link graph;\n",
        "        # dead pages are filtered out as well.\n",
        "        # Links that pass through states (1) through (3) should be members\n",
        "        # of the `enqueued` set; this way, they will not be revisited.\n",
        "        filtered_links = set(filter_links(curr_link, links,\n",
        "                                          blacklist, whitelist,\n",
        "                                          follow_relative_links))\n",
        "        new_links = [href for href in filtered_links\n",
        "                     if unique_url(href) not in enqueued]\n",
        "        sampled_links = sample(new_links,\n",
        "                               min(max_links_per_page, len(new_links)))\n",
        "        for link in filtered_links:\n",
        "            link_graph.append({'from': curr_link, 'to': link})\n",
        "        for link in sampled_links:\n",
        "            link_queue.append(link)\n",
        "            enqueued.add(unique_url(link))\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Crawl finished (saw {len(enqueued)} links)')\n",
        "    filtered = [link for link in link_graph\n",
        "                if (unique_url(link['to']) in enqueued and \n",
        "                   link['to'] not in dead)]\n",
        "    with open(save_file, 'w') as f:\n",
        "        json.dump(\n",
        "            {\n",
        "                'start_url': start_url,\n",
        "                'n': n,\n",
        "                'follow_relative_links': follow_relative_links,\n",
        "                'max_links_per_page': max_links_per_page,\n",
        "                'timestamp': ts,\n",
        "                'blacklist': blacklist,\n",
        "                'whitelist': whitelist,\n",
        "                'links': filtered\n",
        "            }, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QcPJDgX-SPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_crawl_graph(save_file):\n",
        "  \"\"\"Loads crawl data as a NetworkX directed graph.\"\"\"\n",
        "  with open(save_file) as f:\n",
        "    crawl_data = json.load(f)\n",
        "  graph = nx.DiGraph()\n",
        "  graph.add_edges_from([(link['from'], link['to'])\n",
        "                        for link in crawl_data['links']])\n",
        "  for node in graph.nodes:\n",
        "    graph.nodes[node]['url'] = node\n",
        "  graph = nx.relabel_nodes(graph,\n",
        "                           {url: idx for idx, url in enumerate(graph.nodes)})\n",
        "  return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIen7eM6-WTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crawl('https://xkcd.com',\n",
        "       'xkcd.json',  # once you run this example, you've got the output saved in this JSON format\n",
        "        n=200,\n",
        "        follow_relative_links=True,\n",
        "        max_links_per_page=20,\n",
        "        blacklist=('facebook', 'messenger', 'linkedin'))  # avoid link traps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0zfZbNK-jxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " crawl('https://en.wikipedia.org/wiki/Group_theory',\n",
        "       'wiki_math.json',\n",
        "        n=4000,\n",
        "        follow_relative_links=True,\n",
        "        max_links_per_page=20,\n",
        "        whitelist=('en.wikipedia.org/wiki/',),   # this is telling it to stay within wikipedia!\n",
        "        blacklist=('special:', 'talk:', 'user:', 'portal:',\n",
        "                   'help:', 'category:', 'wikipedia:', 'file:'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyC4MAK--4IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph = load_crawl_graph('xkcd.json')  # or whichever JSON you want to build a graph from\n",
        "A = nx.adjacency_matrix(graph).todense()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wgYYgTc_BLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "options = {\n",
        "    'node_color': 'red',\n",
        "    'node_size': 8,\n",
        "}\n",
        "nx.draw(graph, **options)   # there are lots of ways to visualize graphs and do graph statistics in NetworkX"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}